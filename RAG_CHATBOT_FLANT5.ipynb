{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0EWZcSwGCFPo"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPW26kD5L2sQVfYS3eOXjTX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vineela-2315/chatbot_llm-IBM/blob/main/RAG_CHATBOT_FLANT5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Loading the data\n",
        "\n",
        "After completing this lab you will be able to:\n",
        "\n",
        " - Understand how to use `TextLoader` to load **text files.**\n",
        " - Learn how to load **PDFs** using `PyPDFLoader` and `PyMuPDFLoader`.\n",
        " - Use `UnstructuredMarkdownLoader` to load **Markdown files.**(.md files-headings, lists, links, code blocks, etc. are format nicely with symbols)\n",
        " - Load **JSON** files with `JSONLoader` using jq schemas.\n",
        " - Process **CSV** files with `CSVLoader` and `UnstructuredCSVLoader`.\n",
        " - Load **Webpage content** using `WebBaseLoader`.\n",
        " - Load **Word documents** using `Docx2txtLoader`.\n",
        " - Utilize `UnstructuredFileLoader` for **various file types.**"
      ],
      "metadata": {
        "id": "0EWZcSwGCFPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The LangChain team split the project in 2024 so that:\n",
        "langchain → only has core logic (chains, agents, prompts).\n",
        "langchain-community → has integrations (PDF loaders, SQL loaders, Hugging Face embeddings, Pinecone, etc.).\n",
        "'''\n",
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EsHppIQKBaoE",
        "outputId": "0c77e624-0a32-429a-a76d-13c8dd420453"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-core<2.0.0,>=0.3.75 (from langchain_community)\n",
            "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.43)\n",
            "Collecting requests<3,>=2.32.5 (from langchain_community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.16)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.3.75-py3-none-any.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-core, langchain_community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.74\n",
            "    Uninstalling langchain-core-0.3.74:\n",
            "      Successfully uninstalled langchain-core-0.3.74\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-core-0.3.75 langchain_community-0.3.29 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader,PyPDFLoader,PyMuPDFLoader,UnstructuredMarkdownLoader,JSONLoader,CSVLoader,UnstructuredCSVLoader,WebBaseLoader,Docx2txtLoader,UnstructuredFileLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VpZ-XIU-B0GB",
        "outputId": "1434ce6b-ec1a-4e76-ba50-492d247c6c6a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e62d5ce",
        "outputId": "43939b4a-061f-4d34-9b80-f3a486b2d773"
      },
      "source": [
        "%pip install pypdf"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fe730c5",
        "outputId": "e6efaf72-268f-4d91-9a51-f3648db946fa"
      },
      "source": [
        "%pip install docx2txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
            "Downloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "To check certain file exist or not.\n",
        "Summary:\n",
        "os → builds file paths correctly across OS.\n",
        "glob → finds files by pattern (like .txt, .pdf).\n",
        "Together → reliable way to check, count, and list files before loading them in LangChain.\n",
        "\n",
        "txt_files = glob.glob(os.path.join(folder_path, \"*.txt\")) #safe for all types of os.\n",
        "txt_files = glob.glob(\"data/*.txt\")   # might fail on Windows/Linux differences\n",
        "'''\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    CSVLoader,\n",
        "    JSONLoader,\n",
        "    PyPDFLoader,\n",
        "    PyMuPDFLoader, # Added PyMuPDFLoader based on the first markdown cell\n",
        "    UnstructuredMarkdownLoader,\n",
        "    UnstructuredHTMLLoader,\n",
        "    UnstructuredFileLoader,\n",
        "    UnstructuredWordDocumentLoader,\n",
        "    UnstructuredExcelLoader,\n",
        "    UnstructuredPowerPointLoader,\n",
        "    UnstructuredImageLoader,\n",
        "    WebBaseLoader, # Added WebBaseLoader based on the first markdown cell\n",
        "    Docx2txtLoader # Added Docx2txtLoader based on the first markdown cell\n",
        ")\n",
        "def loaders(p):\n",
        "  d=[]\n",
        "  #word\n",
        "  for i in glob.glob(os.path.join(p,\"**/*.docx\"),recursive=True):\n",
        "      l=Docx2txtLoader(i)\n",
        "      d.extend(l.load())\n",
        "  return d\n",
        "'''\n",
        "  #text\n",
        "  for i in glob.glob(os.path.join(p,\"**/*.txt\"),recursive=True):\n",
        "      l = TextLoader(i)\n",
        "      d.extend(l.load())\n",
        "\n",
        "  #webpage\n",
        "  url=[\"https://en.wikipedia.org/wiki/Generative_artificial_intelligence\",\"https://huggingface.co/docs/transformers/index\"]\n",
        "  for i in url:\n",
        "    l=WebBaseLoader(i)\n",
        "    d.extend(l.load())\n",
        "'''\n",
        "\n",
        "path = \"/content/\"\n",
        "d=loaders(path)\n",
        "\n",
        "print(f\"Total text documents loaded: {len(d)}\")\n",
        "if d:\n",
        "   for x, c in enumerate(d):\n",
        "        print(f\"\\n--- Document {x+1} ---\")\n",
        "        print(c.page_content)\n",
        "  #for 1 page\n",
        "  #print(f\"content:{d[0].page_content}\")\n",
        "else:\n",
        "  print(\"no documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AvjUVhRrIYcr",
        "outputId": "57596e7c-f7cc-4875-a856-c6210b501aa4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total text documents loaded: 1\n",
            "\n",
            "--- Document 1 ---\n",
            "VINEELA ANUGURU\n",
            "\n",
            "GenAI Engineer \n",
            "\n",
            "      Email: vineela.anuguru@gmail.com | Phone: +91-9182985631\n",
            "       LinkedIn: linkedin.com/in/Vineela-anuguru-156893337  \n",
            "\n",
            "\n",
            "\n",
            "PROFESSIONAL SUMMARY\n",
            "\n",
            "Results-driven AI & Data professional with around 4 years of hands-on experience across Testing, Data Analytics, Machine Learning, and Generative AI. Proven track record of transforming raw data into actionable insights, building predictive models, and deploying LLM-powered applications to solve real-world business challenges. Skilled in Python, SQL, Scikit-learn, Power BI, and modern GenAI frameworks like LangChain and OpenAI. Passionate about leveraging AI to drive innovation, automation, and business impact.\n",
            "\n",
            "\n",
            "\n",
            "CORE SKILLS\n",
            "\n",
            "Programming: Python, SQL\n",
            "\n",
            "Data science: Data Preprocessing, Machine learning, Deep Learning basics, NLP\n",
            "\n",
            "Gen AI: Transformers, LLMs, Vector DBs, RAG, Finetuning, Prompt Engineering\n",
            "\n",
            "Libraries: Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn, pyTorch, TensorFlow\n",
            "\n",
            "Tools: Power BI, Git, Jupyter\n",
            "\n",
            "Platforms: Azure ML studio, Azure OpenAI studio, Azure Data bricks, Azure Data Lake\n",
            "\n",
            "\n",
            "\n",
            "PROFESSIONAL EXPERIENCE\n",
            "\n",
            "\n",
            "\n",
            "Wipro Ltd-Bangalore, India\n",
            "\n",
            "Oct 2021-present\n",
            "\n",
            "Role: Test Engineer→ Data Scientist → GenAI Engineer    \n",
            "\n",
            "\n",
            "\n",
            "Integrated LangChain and GPT-4 with Azure Cognitive Search to enable context-aware question answering from large QA documents like requirement specs and test plans.\n",
            "\n",
            "Designed and implemented document chunking, embedding pipelines, and vector database indexing to improve retrieval accuracy and response relevance.\n",
            "\n",
            "Developed an NLP pipeline using spaCy NER and regex to extract medical entities (diagnosis, symptoms, vitals, labs) from scanned lab reports and PDF documents.\n",
            "\n",
            "Applied supervised ML models (Random Forest, Logistic Regression) to classify patient cases into departments (Cardiology/Neurology) and assess severity level (Low, Medium, Critical) using clinical keywords and numeric thresholds.\n",
            "\n",
            "Integrated OCR (Tesseract) with preprocessing (noise removal, thresholding) and automated CSV/JSON generation for dashboards and clinical triage tools, reducing manual case review time by 60%.\n",
            "\n",
            "Designed, executed, and maintained over 500+ functional and regression test cases for key trading workflows including order execution, profit/loss tracking, and portfolio updates.\n",
            "\n",
            "Identified, logged, and tracked defects using JIRA, collaborating closely with development teams to ensure timely resolution and high-quality releases.\n",
            "\n",
            "\n",
            "\n",
            "PROJECTS   \n",
            "\n",
            "\n",
            "\n",
            "Q & A chatbot for QA Team                                                                                        Jan 2024-Aug 2025\n",
            "\n",
            "Developed a GenAI-based intelligent assistant that automates QA documentation analysis, reducing manual effort and improving test planning efficiency.\n",
            "\n",
            "Patient360 Intelligence-Clinical Document Summarization (NER, ML)             Aug 2022-Dec 2023\n",
            "\n",
            "Built a clinical data pipeline using OCR, NER, and ML to extract patient info, classify departments and triage severity levels, enabling faster clinical decisions from unstructured reports.\n",
            "\n",
            " Testing of TradePro Banking Platform (Selenium, Jira, Excel)                            Jan 2022-July 2022\n",
            "\n",
            "Conducted end-to-end functional and backend testing of a trading platform to ensure accurate P&L computation and seamless portfolio tracking.\n",
            "\n",
            "\n",
            "\n",
            "EDUCATION\n",
            "\n",
            "Bachelor of Technology (Electronics and Communication Engineering)\n",
            "2017 – 2021\n",
            "KSRM College of Engineering, Kadapa.\n",
            "\n",
            "\n",
            "\n",
            "CERTIFICATIONS\n",
            "\n",
            "Certified AI Foundations Associates from Oracle Cloud Infrastructure 2024(ORACLE)\n",
            "\n",
            "Generative AI with LLMs (DeepLearning.AI)\n",
            "\n",
            "ChatGPT Prompt Engineering for Developers (DeepLearning.AI)\n",
            "\n",
            "GenAI with Azure (Coursera)\n",
            "\n",
            "Introduction to LLMs (Udemy)\n",
            "\n",
            "\n",
            "\n",
            "AWARDS & ACHIEVEMENTS\n",
            "\n",
            "Successfully led GenAI PoC for internal QA Chatbot, adopted by test team in Wipro\n",
            "\n",
            "Maintained 100% on-time delivery record for client AI/ML deliverables over 2+ years\n",
            "\n",
            "Delivered 87 % accurate Models deployed into production\n",
            "\n",
            "\n",
            "\n",
            "DECLARATION\n",
            "\n",
            "                  I hereby declare that the information furnished above is true to the best of my knowledge and belief.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.Textsplitting\n",
        "Why Do We Need Text Splitting?\n",
        "1.Large docs (PDFs, webpages, Word, etc.) can be too big for an embedding model or LLM input.\n",
        "\n",
        "2.Embedding models (e.g., OpenAI’s ada, Cohere, HuggingFace models) have token limits.\n",
        "\n",
        "3.If you embed the whole doc, the embeddings become too general → retrieval fails.\n",
        "\n",
        "Splitting into smaller chunks:\n",
        ">Keeps semantic meaning\n",
        ">Fits into LLM token window\n",
        ">Makes search results more relevant.\n",
        "\n",
        "#Key Concepts of Text Splitting\n",
        "1. Chunking\n",
        "\n",
        "Splitting long text into smaller chunks.\n",
        "\n",
        "Example:\n",
        "Doc = \"Python is a programming language. It is widely used in AI. LangChain makes LLM apps easy.\"\n",
        "\n",
        "If you split into chunks of 20 characters:\n",
        "Chunk1 → \"Python is a programm\"\n",
        "Chunk2 → \"ing language.It is\"\n",
        "\n",
        "2. Chunk Size\n",
        "\n",
        "Maximum length of each split (characters/tokens).\n",
        "Too small → context is lost.\n",
        "Too big → exceeds model limits or embeddings become vague.\n",
        "Sweet spot → usually 500–1000 tokens (depends on task).\n",
        "\n",
        "3. Chunk Overlap\n",
        "\n",
        "Adds an overlap between chunks to preserve context across boundaries.\n",
        "Example: chunk size = 100, overlap = 20.\n",
        "Chunk1 = words 0–100\n",
        "Chunk2 = words 80–180\n",
        "This way, “cut off” sentences don’t lose meaning.\n",
        "\n",
        "4. Splitter Strategies\n",
        "\n",
        "#LangChain provides multiple text splitters:\n",
        "\n",
        "1.CharacterTextSplitter\n",
        "Splits based on character length.\n",
        "Simple but may cut sentences abruptly.\n",
        "\n",
        "2.RecursiveCharacterTextSplitter (most common)\n",
        "Tries to split at:\n",
        "\n",
        "Paragraph → Sentence → Word → Character\n",
        "Maintains natural boundaries (best for RAG).\n",
        "\n",
        "3.TokenTextSplitter\n",
        "Splits by token count (more precise for LLM token limits).\n",
        "\n",
        "4.MarkdownTextSplitter\n",
        "Splits respecting Markdown structure (good for docs, READMEs).\n",
        "\n",
        "5.Language-Specific Splitters\n",
        "\n",
        "PythonCodeSplitter → keeps functions together.\n",
        "\n",
        "HTMLSplitter → respects tags.\n",
        "\n",
        "LatexSplitter, MarkdownSplitter, etc."
      ],
      "metadata": {
        "id": "cpoIWodjAKf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "LF2Kg7TbAQ0q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_splitter(d,chunk_size=500,chunk_overlap=10):\n",
        "  s=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
        "  chunks=s.split_documents(d)\n",
        "  print(f\"Total chunks:{len(chunks)}\")\n",
        "  for i,c in enumerate(chunks):\n",
        "    print(f\"\\nchunks{i+1}:\")\n",
        "    print(c.page_content)\n",
        "  # Return the chunks list\n",
        "  return chunks\n",
        "\n",
        "# Call the function and store the returned chunks in a variable\n",
        "text_chunks = text_splitter(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qobgI7QB4O0",
        "outputId": "7dff093b-14c3-45b1-c80d-2b5691e0d97d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks:11\n",
            "\n",
            "chunks1:\n",
            "VINEELA ANUGURU\n",
            "\n",
            "GenAI Engineer \n",
            "\n",
            "      Email: vineela.anuguru@gmail.com | Phone: +91-9182985631\n",
            "       LinkedIn: linkedin.com/in/Vineela-anuguru-156893337  \n",
            "\n",
            "\n",
            "\n",
            "PROFESSIONAL SUMMARY\n",
            "\n",
            "chunks2:\n",
            "Results-driven AI & Data professional with around 4 years of hands-on experience across Testing, Data Analytics, Machine Learning, and Generative AI. Proven track record of transforming raw data into actionable insights, building predictive models, and deploying LLM-powered applications to solve real-world business challenges. Skilled in Python, SQL, Scikit-learn, Power BI, and modern GenAI frameworks like LangChain and OpenAI. Passionate about leveraging AI to drive innovation, automation, and\n",
            "\n",
            "chunks3:\n",
            "and business impact.\n",
            "\n",
            "chunks4:\n",
            "CORE SKILLS\n",
            "\n",
            "Programming: Python, SQL\n",
            "\n",
            "Data science: Data Preprocessing, Machine learning, Deep Learning basics, NLP\n",
            "\n",
            "Gen AI: Transformers, LLMs, Vector DBs, RAG, Finetuning, Prompt Engineering\n",
            "\n",
            "Libraries: Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn, pyTorch, TensorFlow\n",
            "\n",
            "Tools: Power BI, Git, Jupyter\n",
            "\n",
            "Platforms: Azure ML studio, Azure OpenAI studio, Azure Data bricks, Azure Data Lake\n",
            "\n",
            "\n",
            "\n",
            "PROFESSIONAL EXPERIENCE\n",
            "\n",
            "\n",
            "\n",
            "Wipro Ltd-Bangalore, India\n",
            "\n",
            "Oct 2021-present\n",
            "\n",
            "chunks5:\n",
            "Role: Test Engineer→ Data Scientist → GenAI Engineer    \n",
            "\n",
            "\n",
            "\n",
            "Integrated LangChain and GPT-4 with Azure Cognitive Search to enable context-aware question answering from large QA documents like requirement specs and test plans.\n",
            "\n",
            "Designed and implemented document chunking, embedding pipelines, and vector database indexing to improve retrieval accuracy and response relevance.\n",
            "\n",
            "chunks6:\n",
            "Developed an NLP pipeline using spaCy NER and regex to extract medical entities (diagnosis, symptoms, vitals, labs) from scanned lab reports and PDF documents.\n",
            "\n",
            "Applied supervised ML models (Random Forest, Logistic Regression) to classify patient cases into departments (Cardiology/Neurology) and assess severity level (Low, Medium, Critical) using clinical keywords and numeric thresholds.\n",
            "\n",
            "chunks7:\n",
            "Integrated OCR (Tesseract) with preprocessing (noise removal, thresholding) and automated CSV/JSON generation for dashboards and clinical triage tools, reducing manual case review time by 60%.\n",
            "\n",
            "Designed, executed, and maintained over 500+ functional and regression test cases for key trading workflows including order execution, profit/loss tracking, and portfolio updates.\n",
            "\n",
            "chunks8:\n",
            "Identified, logged, and tracked defects using JIRA, collaborating closely with development teams to ensure timely resolution and high-quality releases.\n",
            "\n",
            "\n",
            "\n",
            "PROJECTS   \n",
            "\n",
            "\n",
            "\n",
            "Q & A chatbot for QA Team                                                                                        Jan 2024-Aug 2025\n",
            "\n",
            "Developed a GenAI-based intelligent assistant that automates QA documentation analysis, reducing manual effort and improving test planning efficiency.\n",
            "\n",
            "chunks9:\n",
            "Patient360 Intelligence-Clinical Document Summarization (NER, ML)             Aug 2022-Dec 2023\n",
            "\n",
            "Built a clinical data pipeline using OCR, NER, and ML to extract patient info, classify departments and triage severity levels, enabling faster clinical decisions from unstructured reports.\n",
            "\n",
            " Testing of TradePro Banking Platform (Selenium, Jira, Excel)                            Jan 2022-July 2022\n",
            "\n",
            "chunks10:\n",
            "Conducted end-to-end functional and backend testing of a trading platform to ensure accurate P&L computation and seamless portfolio tracking.\n",
            "\n",
            "\n",
            "\n",
            "EDUCATION\n",
            "\n",
            "Bachelor of Technology (Electronics and Communication Engineering)\n",
            "2017 – 2021\n",
            "KSRM College of Engineering, Kadapa.\n",
            "\n",
            "\n",
            "\n",
            "CERTIFICATIONS\n",
            "\n",
            "Certified AI Foundations Associates from Oracle Cloud Infrastructure 2024(ORACLE)\n",
            "\n",
            "Generative AI with LLMs (DeepLearning.AI)\n",
            "\n",
            "ChatGPT Prompt Engineering for Developers (DeepLearning.AI)\n",
            "\n",
            "chunks11:\n",
            "GenAI with Azure (Coursera)\n",
            "\n",
            "Introduction to LLMs (Udemy)\n",
            "\n",
            "\n",
            "\n",
            "AWARDS & ACHIEVEMENTS\n",
            "\n",
            "Successfully led GenAI PoC for internal QA Chatbot, adopted by test team in Wipro\n",
            "\n",
            "Maintained 100% on-time delivery record for client AI/ML deliverables over 2+ years\n",
            "\n",
            "Delivered 87 % accurate Models deployed into production\n",
            "\n",
            "\n",
            "\n",
            "DECLARATION\n",
            "\n",
            "                  I hereby declare that the information furnished above is true to the best of my knowledge and belief.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Embedding\n",
        "\n",
        "Why do we need embeddings in RAG?\n",
        "In a QA chatbot with RAG:\n",
        "\n",
        "We use an embedding model to convert each chunk into a vector.\n",
        "When a user asks a question, we also convert the question into a vector.\n",
        "We search for similar vectors (chunks) using FAISS/Pinecone/Chroma.\n",
        "The most relevant chunks are retrieved and given to the LLM to answer.\n",
        "\n",
        "#How?\n",
        "The Problem\n",
        "Computers only understand numbers, not text.\n",
        "But words like “king”, “queen”, “man”, “woman” have meanings and relationships.\n",
        "We want a way to map words into numbers so that similar words → similar numbers.\n",
        "\n",
        "#Traditional methods:\n",
        "1.one hot encoding(1,0,0,0)\n",
        "2.word2vec-king=[0.56],queen=[0.45],man=[0.34],woman=[0.23]\n",
        "3.Modern Embeddings (Transformers):\n",
        "BERT, Sentence Transformers, OpenAI embeddings go beyond word-level.\n",
        "They embed sentences, paragraphs, and documents into vectors.\n",
        "They capture context → e.g., the word “bank” in “river bank” vs “money bank” will have different embeddings.\n",
        "\n",
        "#Analogy\n",
        "Think of embeddings like GPS coordinates for words:\n",
        "Each word gets an “address” in a semantic space.\n",
        "Words with similar meanings are neighbors (close coordinates).\n",
        "Example: doctor and nurse will be close, but doctor and banana will be far apart.\n",
        "\n",
        "#used:\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "1️⃣ What it is:\n",
        "Type: Transformer-based model\n",
        "Library: sentence-transformers (built on top of Hugging Face Transformers)\n",
        "Purpose: Generate dense vector embeddings for sentences or text snippets.\n",
        "Model Size: Small and efficient (MiniLM) → faster than BERT while still accurate.\n",
        "\n",
        "2️⃣ Architecture\n",
        "MiniLM: Miniature version of BERT-like transformers\n",
        "Lightweight but retains semantic understanding.\n",
        "Fewer parameters → faster inference and lower memory usage.\n",
        "L6: 6 Transformer layers (BERT-base has 12 layers).\n",
        "v2: Improved version with better embedding quality.\n",
        "\n",
        "3️⃣ Use Cases\n",
        "\n",
        "Semantic Search / RAG\n",
        "Convert documents and queries into embeddings\n",
        "Find documents similar to a query by comparing vectors\n",
        "Clustering\n",
        "Group similar sentences or documents\n",
        "Paraphrase Detection\n",
        "Compare sentence embeddings for similarity\n",
        "Text Classification\n",
        "Use embeddings as input features to ML classifiers\n",
        "\n",
        "4️⃣ Input & Output\n",
        "\n",
        "Input: Single sentence or batch of sentences\n",
        "sentences = [\"I love AI\", \"I enjoy machine learning\"]\n",
        "Output: Fixed-size vector (768 dimensions) for each sentence\n",
        "[0.12, -0.34, 0.87, ..., 0.45]  # length 384 or 768 depending on version\n",
        "Vectors can then be used in cosine similarity, clustering, or ML models.\n",
        "\n",
        "5️⃣ Why it’s popular\n",
        "\n",
        "Efficient: Smaller than BERT → fast embeddings generation\n",
        "Accurate: Good semantic understanding for sentences\n",
        "Easy integration: Works seamlessly with sentence-transformers and LangChain RAG pipelines"
      ],
      "metadata": {
        "id": "Priwd4Vap1SY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create Document objects\n",
        "from langchain.schema import Document\n",
        "# Use the text_chunks variable which contains the list of Document objects\n",
        "list_of_documents = text_chunks\n",
        "\n",
        "#Embedding\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "v=FAISS.from_documents(list_of_documents,embeddings)\n",
        "\n",
        "print(\"FAISS vector store created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZEsw-Ajfpgq",
        "outputId": "d7da30e3-2dc2-4977-d39a-cecbfb24bc21"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS vector store created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#view embeddings\n",
        "doc_text = list_of_documents[10].page_content\n",
        "print(\"Document Text:\", doc_text[:2000], \"...\")\n",
        "\n",
        "embedding_vector = embeddings.embed_query(doc_text)\n",
        "print(\"Embedding length:\", len(embedding_vector))\n",
        "print(\"First 10 numbers of embedding:\",embedding_vector[:400])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CGv-yIS0cGZ",
        "outputId": "576a63f1-d9cd-4c32-c06e-9b36ea071d35"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Text: GenAI with Azure (Coursera)\n",
            "\n",
            "Introduction to LLMs (Udemy)\n",
            "\n",
            "\n",
            "\n",
            "AWARDS & ACHIEVEMENTS\n",
            "\n",
            "Successfully led GenAI PoC for internal QA Chatbot, adopted by test team in Wipro\n",
            "\n",
            "Maintained 100% on-time delivery record for client AI/ML deliverables over 2+ years\n",
            "\n",
            "Delivered 87 % accurate Models deployed into production\n",
            "\n",
            "\n",
            "\n",
            "DECLARATION\n",
            "\n",
            "                  I hereby declare that the information furnished above is true to the best of my knowledge and belief. ...\n",
            "Embedding length: 384\n",
            "First 10 numbers of embedding: [-0.07766377925872803, -0.04160761833190918, -0.024703199043869972, 0.011875772848725319, -0.03259938955307007, -0.02960122376680374, 0.02360340766608715, -0.026876408606767654, 0.015337335877120495, 0.07143209874629974, 0.030869893729686737, -0.0018991498509421945, 0.05734168738126755, 0.03758826106786728, 0.05762194097042084, 0.0706072673201561, 0.0925658792257309, -0.0977880135178566, 0.01757633127272129, -0.024884860962629318, -0.00580486049875617, 0.08150684088468552, 0.05637529492378235, -0.017966514453291893, 0.04762181267142296, -0.05310516059398651, -0.041153259575366974, 0.05732918158173561, 0.04432357847690582, -0.070548415184021, -0.004137041047215462, 0.06667602062225342, 0.025542214512825012, 0.03576766699552536, 0.05670204386115074, 0.034845978021621704, -0.023596590384840965, -0.0057266633957624435, -0.0329812653362751, -0.04569583758711815, 0.03980614244937897, -0.02087053656578064, -0.03518405184149742, 0.015310709364712238, 0.11110961437225342, -0.01300861220806837, -0.08703195303678513, 0.003319208510220051, -0.0719156488776207, 0.08004119247198105, -0.07381406426429749, -0.062094226479530334, 0.1011640727519989, 0.04620818793773651, -0.05074724555015564, 0.011436089873313904, 0.029532674700021744, -0.02935604378581047, 0.007602894678711891, -0.013523886911571026, -0.023484790697693825, -0.07739793509244919, -0.056805435568094254, -0.017329853028059006, 0.011963078752160072, 0.008989783003926277, -0.027364399284124374, 0.031621310859918594, -0.010624576359987259, -0.07513616234064102, 0.036900438368320465, -0.04318720102310181, -0.023431794717907906, 0.035136330872774124, -0.010486114770174026, 0.06634844094514847, 0.026858678087592125, 0.03631238266825676, 0.08609190583229065, 0.009334425441920757, 0.004963731858879328, 0.011671395972371101, -0.0038303607143461704, 0.03784799575805664, -0.012748124077916145, -0.015024688094854355, 0.08605602383613586, 0.009631860069930553, 0.002051051240414381, -0.05469944328069687, -0.010893307626247406, -0.059569086879491806, 0.036654032766819, 0.06846088916063309, 0.020877836272120476, -0.01603846438229084, -0.053793828934431076, -0.06431061029434204, -0.018673785030841827, 0.04256339743733406, -0.053559381514787674, 0.026765981689095497, -0.09662388265132904, -0.05456920713186264, -0.04708438366651535, 0.006205786485224962, 0.05244603380560875, 0.027717750519514084, 0.06028123199939728, 0.056697893887758255, -0.0562877394258976, 0.042088236659765244, 0.02649758756160736, 0.016902025789022446, 0.0708298459649086, 0.012418988160789013, -0.013592780567705631, 0.12213173508644104, 0.0560130700469017, -0.0430026538670063, -0.02332974411547184, -0.04479179158806801, 0.01090890821069479, -0.04470639303326607, 0.037826668471097946, 0.011379384435713291, -0.027531461790204048, 6.46660748222449e-33, 0.07469142228364944, 0.013896736316382885, -0.035505738109350204, 0.035952720791101456, 0.08174753189086914, 0.022839533165097237, 0.04813913255929947, 0.011765697970986366, 0.003586720209568739, -0.0704360231757164, -0.14106503129005432, 0.07413402944803238, -0.04806235432624817, 0.023429276421666145, 0.027877185493707657, -0.03283987566828728, -0.040067482739686966, 0.035471707582473755, 0.06336338073015213, 0.054017145186662674, 0.06605270504951477, -0.042644061148166656, 0.06681296974420547, -0.0345078743994236, 0.07210443913936615, 0.10635951161384583, 0.09050706773996353, -0.016164593398571014, 0.08642605692148209, 0.043795496225357056, -0.035598982125520706, -0.03729311749339104, 0.003402224276214838, -0.03580651432275772, 0.03863023594021797, 0.015239667147397995, -0.12087328732013702, -0.01780022494494915, 0.02090761438012123, 0.05572788044810295, -0.016229916363954544, 0.0024099741131067276, 0.0044632540084421635, -0.08099077641963959, -0.08846892416477203, 0.00395815959200263, -0.016608120873570442, -0.012340056709945202, 0.0469558946788311, 0.022148892283439636, -0.07889032363891602, 0.013142756186425686, -0.10199970752000809, -0.01716363988816738, 0.022517496719956398, -0.11340209096670151, 0.019476477056741714, -0.011690334416925907, -0.04089747741818428, 0.009535488672554493, -0.05684542655944824, -0.043826594948768616, -0.020953664556145668, 0.022566573694348335, -0.019458625465631485, -0.03705670312047005, -0.06046688184142113, -0.019922826439142227, 0.055151548236608505, -0.025922931730747223, 0.057558074593544006, -0.02190546691417694, -0.02535059303045273, -0.026920132339000702, -0.06211052089929581, -0.07990726828575134, 0.013107729144394398, -0.02882372960448265, 0.036824312061071396, -0.015379248186945915, -0.055110301822423935, 0.04773052781820297, -0.045672472566366196, -0.022118043154478073, -0.013698446564376354, -0.040168892592191696, 0.024344583973288536, -0.021576710045337677, -0.03167743980884552, 0.04136509448289871, -0.009189428761601448, 0.0034523385111242533, 0.03367777168750763, 0.03938286378979683, -0.05181552842259407, -4.9005911328559915e-33, -0.0792677029967308, 0.06070810183882713, -0.06493009626865387, 0.11055319011211395, 0.07189449667930603, -0.12819978594779968, -0.016447896137833595, 0.10218064486980438, 0.0524723157286644, 0.04712025076150894, 0.007049381732940674, -0.021042222157120705, -0.014483545906841755, 0.059759825468063354, -0.023472264409065247, 0.005588945001363754, 0.055568523705005646, -0.08800803869962692, 0.009128023870289326, -0.03948395326733589, 0.027644239366054535, 0.06190348044037819, -0.03850041702389717, -0.03930428996682167, 0.04534412547945976, 0.023954052478075027, -0.01972665823996067, 0.06743598729372025, -0.03895491734147072, -0.05145655944943428, 0.009859634563326836, -0.037351176142692566, -0.0924796387553215, 0.0458245649933815, 0.006335726473480463, 0.022595133632421494, 0.1131083071231842, 0.006731071509420872, 0.03674852102994919, 0.03182176128029823, 0.11301873624324799, -0.03239035606384277, -0.14860020577907562, 0.022555718198418617, 0.009264183230698109, -0.06627612560987473, -0.05600408464670181, 0.00497454172000289, 0.060926783829927444, 0.000352526520146057, 0.0051257433369755745, 0.020807217806577682, 0.06347089260816574, -0.017123732715845108, -0.06200903281569481, -0.024140484631061554, 0.018779993057250977, 0.0021354074124246836, -0.11360779404640198, 0.06349057704210281, 0.04414751008152962, -0.010799288749694824, 0.024121297523379326, 0.028890887275338173, 0.02520175091922283, 0.06569688767194748, 0.06954491138458252, 0.0728018656373024, -0.03630823642015457, -0.060641832649707794, 0.009394777938723564, -0.020054515451192856, -0.08049225807189941, -0.022633884102106094, -0.0014301493065431714, 0.002032950986176729, -0.09002724289894104, 0.008927165530622005, 0.04696924611926079, -0.10285685956478119, -0.0468488484621048, -0.013226287439465523, -0.022300664335489273, 0.06966034322977066, 0.013871396891772747, 0.03667176887392998, 0.13747341930866241, -0.04612332209944725, -0.03127922862768173, 0.08784887194633484, -0.03052600473165512, 0.023975588381290436, -0.036305997520685196, 0.018246617168188095, -0.015425968915224075, -4.0503117304524494e-08, -0.016802195459604263, -0.07286638766527176, -0.02944229729473591, 0.02206617034971714, -0.002877645893022418, -0.018058886751532555, -0.06002214178442955, 0.04833965748548508, 0.03211131691932678, 0.04579727351665497, -0.011023915372788906, -0.005460462067276239, -0.10584315657615662, 0.013160748407244682, 0.09003844112157822, 0.03299759700894356, -0.004323930013924837, 0.0783914104104042, -0.06849376857280731, -0.1521967053413391, 0.08125419169664383, 0.07255041599273682, 0.04458945244550705, -0.03140687197446823, -0.07523224502801895, -0.02222864329814911, 0.00018331586034037173, 0.07517322152853012, -0.03014739230275154, 0.004646360874176025, -0.006733168847858906, -0.023959515616297722, -0.011074647307395935, -0.06643670797348022, 0.04171103611588478, 0.023421525955200195, 0.015269326977431774, -0.053315419703722, 0.04171602800488472, 0.0668000876903534, -0.03779866546392441, 0.00020725461945403367, -0.00853337999433279, 0.0170559324324131, 0.00022118979541119188, -0.02064688131213188, -0.11849139630794525, -0.1233116015791893, -0.033385708928108215, -0.004324037581682205, -0.0344073511660099, -0.028521426022052765, 0.010814055800437927, 0.04727935418486595, 0.011506825685501099, 0.04055895283818245, 0.013496780768036842, -0.12196394801139832, 0.029974061995744705, 0.08292540907859802, 0.0214456245303154, 0.004325231537222862, 0.01627393066883087, 0.00604258943349123]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.Query the Vector Store (Similarity Search)\n",
        "\n",
        "When a user asks a question,\n",
        "1.we:Embed the query using the same embedding model.\n",
        "2.Compare it against all stored vectors.\n",
        "3.Retrieve the most relevant chunks.\n"
      ],
      "metadata": {
        "id": "Hgb6ADIG3SMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What experience does Vineela have in Generative AI?\"\n",
        "#1.convert query to embedding\n",
        "query_vector = embeddings.embed_query(query)\n",
        "\n",
        "print(\"Query vector (first 10 numbers):\")\n",
        "for i in query_vector[:10]:\n",
        "    print(i)\n",
        "\n",
        "#2.do similarity search\n",
        "docs = v.similarity_search(query, k=3)   # k = top 3 most similar chunks\n",
        "\n",
        "for i, doc in enumerate(docs, start=1):\n",
        "    print(f\"\\n--- Result {i} ---\")\n",
        "    print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDdgpM7-3T8w",
        "outputId": "8d0ea37d-b348-4a0f-d5bd-1103fe3cc0c1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query vector (first 10 numbers):\n",
            "-0.06450623273849487\n",
            "-0.03404312953352928\n",
            "-0.021131932735443115\n",
            "0.04558712616562843\n",
            "-0.08666884899139404\n",
            "0.05452815070748329\n",
            "0.021416565403342247\n",
            "0.013445291668176651\n",
            "-0.05660019814968109\n",
            "-0.004076216369867325\n",
            "\n",
            "--- Result 1 ---\n",
            "Results-driven AI & Data professional with around 4 years of hands-on experience across Testing, Data Analytics, Machine Learning, and Generative AI. Proven track record of transforming raw data into actionable insights, building predictive models, and deploying LLM-powered applications to solve real-world business challenges. Skilled in Python, SQL, Scikit-learn, Power BI, and modern GenAI frameworks like LangChain and OpenAI. Passionate about leveraging AI to drive innovation, automation, and\n",
            "\n",
            "--- Result 2 ---\n",
            "VINEELA ANUGURU\n",
            "\n",
            "GenAI Engineer \n",
            "\n",
            "      Email: vineela.anuguru@gmail.com | Phone: +91-9182985631\n",
            "       LinkedIn: linkedin.com/in/Vineela-anuguru-156893337  \n",
            "\n",
            "\n",
            "\n",
            "PROFESSIONAL SUMMARY\n",
            "\n",
            "--- Result 3 ---\n",
            "Identified, logged, and tracked defects using JIRA, collaborating closely with development teams to ensure timely resolution and high-quality releases.\n",
            "\n",
            "\n",
            "\n",
            "PROJECTS   \n",
            "\n",
            "\n",
            "\n",
            "Q & A chatbot for QA Team                                                                                        Jan 2024-Aug 2025\n",
            "\n",
            "Developed a GenAI-based intelligent assistant that automates QA documentation analysis, reducing manual effort and improving test planning efficiency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.use llm to generate output\n",
        "Why Do We Still Need an LLM?\n",
        "1. Chunks ≠ Final Answer\n",
        "FAISS + embeddings only give you the most relevant text pieces.\n",
        "Example:\n",
        "Query → \"Summarize Vineela’s skills.\"\n",
        "FAISS might return chunks like:\n",
        "\n",
        "#without llm\n",
        "\"Skilled in Python, SQL, Scikit-learn…\"\n",
        "\"Experience in Data Analytics, Machine Learning…\"\n",
        "\"Worked with LangChain, OpenAI…\"\n",
        "These are raw pieces, not a proper summary.\n",
        "\n",
        "\n",
        "2. LLM = Synthesizer / Composer\n",
        "\n",
        "The LLM’s job is to read those 3 chunks and generate a coherent, natural-language answer.\n",
        "Without the LLM, the user would see disjointed raw chunks.\n",
        "\n",
        "#With the LLM, they get something like:\n",
        "“Vineela is skilled in Python, SQL, Scikit-learn, and Power BI, with experience in data analytics, machine learning, and GenAI frameworks like LangChain and OpenAI.”\n",
        "\n",
        "3. Handling User Queries\n",
        "\n",
        "Users don’t always ask “give me the chunk”.\n",
        "They may ask:\n",
        "“Summarize Vineela’s skills” (needs summarization).\n",
        "“What GenAI tools has she worked on?” (needs filtering).\n",
        "“Write this in 2 sentences” (needs paraphrasing).\n",
        "Embeddings+FAISS only find where the info is.\n",
        "\n",
        "LLM decides how to express it.\n",
        "\n",
        "✅ Analogy:\n",
        "Think of FAISS as a librarian who fetches the right 3 books for your question.\n",
        "The LLM is the teacher who reads those books and gives you a clear, direct answer instead of dumping the books on your desk.\n",
        "\n",
        "Here we used:\n",
        "1. Flan-T5 (Base / Large / XL) ✅ (Best for CPU / lightweight)\n",
        "Model: \"google/flan-t5-base\" or \"google/flan-t5-large\"\n",
        "Type: Instruction-tuned text2text model.\n",
        "Pros:\n",
        "Small, fast, and free.\n",
        "Excellent for summarization & Q&A.\n",
        "Works well with your MiniLM embeddings in FAISS.\n",
        "Cons:\n",
        "Not conversational, just Q&A."
      ],
      "metadata": {
        "id": "S82DWEnb6Q_4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edf3628e",
        "outputId": "cec6e824-2ce8-4e10-ee74-9f6854f23183"
      },
      "source": [
        "%pip install langchain-huggingface transformers"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.3.75)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.21.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.8)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.16)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.11.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.3.1)\n",
            "Downloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: langchain-huggingface\n",
            "Successfully installed langchain-huggingface-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.generate a llm\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "generator=pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    max_length=512,\n",
        "    temperature=0\n",
        ")\n",
        "#2.wrap the model so that This makes our HuggingFace model (generator) behave like a LangChain LLM.\n",
        "#Now you can use it inside any LangChain chain (RetrievalQA, ConversationalChain, etc).\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generator)\n",
        "\n",
        "#3.Now creat retrival chain\n",
        "'''\n",
        "llm=llm → the Flan-T5 model we wrapped above.\n",
        "retriever=v.as_retriever() → turns your FAISS vector DB into a retriever.\n",
        "When you ask a question, it fetches the top-k most relevant chunks using embeddings.\n",
        "chain_type=\"stuff\" → tells LangChain to stuff (concatenate) all retrieved documents into a single prompt and pass them to the LLM.\n",
        "Other options are \"map_reduce\", \"refine\", but \"stuff\" is the simplest and most common.\n",
        "So this chain = Retriever + LLM → Final Answer.\n",
        "'''\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa=RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=v.as_retriever(),\n",
        "    chain_type=\"stuff\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF96SEYr6wsR",
        "outputId": "a094b5a1-0edc-4935-e33c-dafb9e02dad9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Summarize Vineela's skills.\"\n",
        "answer = qa.run(query)\n",
        "print(\"Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC_m7Lhd-cMG",
        "outputId": "77f91d53-8576-4277-b652-114d1f241102"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3828214142.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  answer = qa.run(query)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Identified, logged, and tracked defects using JIRA, collaborating closely with development teams to ensure timely resolution and high-quality releases. PROJECTS Q & A chatbot for QA Team Jan 2024-Aug 2025 Developed a GenAI-based intelligent assistant that automates QA documentation analysis, reducing manual effort and improving test planning efficiency. CORE SKILLS Programming: Python, SQL Data science: Data Preprocessing, Machine learning, Deep Learning basics, NLP Gen AI: Transformers, LLMs, Vector DBs, RAG, Finetuning, Prompt Engineering Libraries: Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn, pyTorch, TensorFlow Tools: Power BI, Git, Jupyter Platforms: Azure ML studio, Azure OpenAI studio, Azure Data bricks, Azure Data Lake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"In year 2024,vineela worked on which project,give me the project name\"\n",
        "answer=qa.run(query)\n",
        "print(\"Answer:\",answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-CZ1WTI-4Hc",
        "outputId": "ae366d66-cded-4a93-e95d-0605997097cf"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Q & A chatbot for QA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"In year 2022,vineela worked on which project,give me the project name\"\n",
        "answer=qa.run(query)\n",
        "print(\"Answer:\",answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGBKSeJ8_Xpc",
        "outputId": "3366b321-1e1e-4b6a-f3f4-281bd62d058d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Q & A chatbot for QA Team Jan 2024-Aug 2025 Developed a GenAI-based intelligent assistant that automates QA documentation analysis, reducing manual effort and improving test planning efficiency. GenAI with Azure (Coursera) Introduction to LLMs (Udemy) AWARDS & ACHIEVEMENTS Successfully led GenAI PoC for internal QA Chatbot, adopted by test team in Wipro Maintained 100% on-time delivery record for client AI/ML deliverables over 2+ years Delivered 87 % accurate Models deployed into production\n"
          ]
        }
      ]
    }
  ]
}